{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Iterative Importance Sampling : concepts and simple example\n",
    "## A Bayesian method\n",
    "\n",
    "Classically, one ensemble, prior sampling, likelihood weighting : posterior estimates\n",
    "\n",
    "posterior $\\propto$ likelihood * prior \n",
    "\n",
    "p( x | o ) $\\propto$ p( o | x ) * b(x)\n",
    "\n",
    "in the Gaussian case:\n",
    "$$ p( o | x ) \\propto e^{-0.5 (\\frac{x-o}{ \\sigma_o}) ^2}$$\n",
    "\n",
    "Basic procedure:\n",
    "\n",
    "- sample x from prior b(x)\n",
    "- calculate weights $w_i$ = p ( o | $x_i$ )  \n",
    "- estimate quantities carrying weights around\n",
    "- mean = $\\sum_i x_i w_i $\n",
    "\n",
    "Or alternatively, instead of carrying weights, do some resampling\n",
    "\n",
    "- posterior estimates of 3 members: 1 (w=0.2), 2(w=0.5), 3 (w=0.3) \n",
    "- equivalent resampled ensemble (10 members) : [1, 1, 2, 2, 2, 2, 2, 3, 3, 3]\n",
    "- mean = $\\sum_{i=1,10} x_i $\n",
    "\n",
    "The main problem of such an approach, is that if the prior belief is too far off the posterior, the weights may all equal zero, or be concentrated on a few samples only, making a poor approximation of the PDF. The problem is especially acute when exploring a high-dimensional parameter space (since there is an exponential relationship \n",
    "between the number of parameters and the degrees of freedom).\n",
    "\n",
    "The idea behind `iterative importance sampling` is to repeat such weighting/resampling procedure iteratively using intermediate, or `bridging` distributions, so that the sequence of distributions sampled by the particles at each step smoothly converges toward the posterior (this has much bearing with Monte Carlo Markov Chains, except that\n",
    "here we work with an `ensemble` of `particles` at each iteration step). A challenge will be do design the weighting to avoid an over-concentration of weights, a problem sometimes called `ensemble collapse` or degenerescence.\n",
    "\n",
    "Annan and Hargreaves (2010) proposed an iterative importance sampling (IIS) scheme where the weighting function is a scaled version of the posterior, via an exponent $\\epsilon$ (0.05 by default). Moreover, each weighting/resampling step will be counterbalanced by addition of noise, or `jitter`. The noise is chosen as a (multivariate) normal approximation of the ensemble, but with variance scaled down by the same factor $\\epsilon$.\n",
    "\n",
    "Each iteration, two steps:\n",
    "- weighting/re-sampling using $f^\\epsilon $  \n",
    "  where $ f = p( x | o ) \\propto p( o | x ) b(x) $\n",
    "- addition of jitter $N(0, \\epsilon \\Sigma)$ \n",
    "  where $\\Sigma$ is the covariance matrix of the resampled ensemble.\n",
    "\n",
    "In the Gaussian case, these two steps correspond to the folling operations on the PDF of any `proposal` distribution $\\phi$:\n",
    "- weighting by $f^\\epsilon$ : $\\phi \\rightarrow \\phi f^\\epsilon$ (does not require f to be gaussian)\n",
    "- jittering : $\\phi \\rightarrow \\phi f^\\frac{1}{1+\\epsilon}$\n",
    "\n",
    "It is not too difficult to prove that if one first sample the model parameters from a `proposal` distribution $g$,\n",
    "the ensemble will follow at each time step $i$ the distribution:\n",
    "\n",
    "$$\\phi_i = g^{1-\\alpha_i} f^\\alpha_i$$\n",
    "\n",
    "where $\\alpha_i$ is a series that verifies:\n",
    "\n",
    "$$\\alpha_{i+1} = \\frac{\\alpha_i + \\epsilon}{1 + \\epsilon}$$\n",
    "    \n",
    "with $\\alpha_0 = 0$. The series converge and their limits are:\n",
    "\n",
    "$$\\alpha_i \\rightarrow 1$$\n",
    "$$\\phi_i \\rightarrow f = p( x | o )$$\n",
    "\n",
    "## A numerical example (almost without iis module)\n",
    "\n",
    "The following part of this notebook presents a simple example for \n",
    "the convergence of a series of distributions with successive\n",
    "weighting and jittering (variance increase) steps. The iis module\n",
    "does not much more than providing a simple user interface including \n",
    "multivariate models, and provides a couple of handy functions for \n",
    "plotting and diagnostics. Of course, `iis` module could be extended with new \n",
    "methods etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform, lognorm\n",
    "from iis.resampling import multinomial_resampling, residual_resampling # , stratified_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Initialization\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# ensemble size\n",
    "size = 500\n",
    "\n",
    "# posterior distribution (the truth)\n",
    "posterior = norm(loc=0, scale=1) \n",
    "#posterior = lognorm(1) # log-normal distribution : test deviation from theoretical gaussian case\n",
    "\n",
    "# proposal distribution from which to start the process (no prior here)\n",
    "proposal = uniform(loc=0, scale=100)\n",
    "\n",
    "# initialize (or reset) ensemble and a few other things \n",
    "ensemble = proposal.rvs(size=size)\n",
    "ensembles = [] # initialize ensembles\n",
    "alpha = 0\n",
    "alphas = []\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# estimation (can be executed as many times as needed)\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# algorithm parameters\n",
    "epsilon = 0.05  # exponent\n",
    "\n",
    "# number of iterations\n",
    "iterations = 60\n",
    "\n",
    "# sampling method (important !)\n",
    "#\n",
    "# - random_walk : otherwise called multinomial sampling : non-deterministic\n",
    "# - residual_sampling : partly deterministic (main 60% or so just copied, then random walk through residual weights)\n",
    "# - stratified_sampling : NOT IMPLEMENTED (used by Annan and Hargreave, 2010). \n",
    "# - deterministic_sampling : NOT IMPLEMENTED starts like residual sampling, but aggregate residual weights into \n",
    "#    larger particules (called support particles) according to a recursive partition of space, along a user-input \n",
    "#    preferred sub-space. See Li et al (2012) in Signal Processing, doi:10.1016/j.sigpro.2011.12.019\n",
    "#\n",
    "resample_ensemble = residual_resampling  \n",
    "\n",
    "for iteration in xrange(iterations):\n",
    "    ensembles.append(ensemble) # ensemble before update, for the record\n",
    "    alphas.append(alpha)\n",
    "    \n",
    "    # weight the samples according to a scaled version of the likelihood\n",
    "    weights = np.exp(posterior.logpdf(ensemble)*epsilon)\n",
    "    weights /= weights.sum() # normalized weights\n",
    "    \n",
    "    # resample according to weights\n",
    "    ids = resample_ensemble(weights, ensemble.size)\n",
    "    ensemble = ensemble[ids]  # new ensemble \n",
    "    \n",
    "    # add jitter\n",
    "    variance = np.var(ensemble) # after resampling\n",
    "    jitter = np.random.normal(loc=0, scale=np.sqrt(variance*epsilon), size=ensemble.size)\n",
    "    ensemble += jitter\n",
    "    \n",
    "    # diagnostic only: update alpha to check convergence properties\n",
    "    # supposedly sampled distribution will be: proposal**(1-alpha) * posterior**alpha\n",
    "    alpha = (alpha + epsilon)/(1+epsilon)\n",
    "    \n",
    "    #if alpha > 0.99999: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# check results\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "bins = 10\n",
    "\n",
    "mi, ma = -20, 20\n",
    "x = np.linspace(mi, ma, 1000)\n",
    "\n",
    "# PDF\n",
    "ax = axes[0]\n",
    "ax.hist(ensemble, bins, histtype='step', label='result', normed=True)\n",
    "ax.plot(x, posterior.pdf(x), label='truth')\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\"Histogram (empirical PDF)\")\n",
    "\n",
    "# CDF\n",
    "ax = axes[1]\n",
    "ax.hist(ensemble, bins, histtype='step', label='result', normed=True, cumulative=True)\n",
    "ax.plot(x, posterior.cdf(x), label='true posterior')\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\"Empirical cumulative distribution function (CDF))\")\n",
    "\n",
    "fig.set_figwidth(17)\n",
    "ax.set_xlim([mi, ma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# check convergence properties\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pct = [50, 5, 95, 16, 84]\n",
    "n = len(ensembles)\n",
    "data = np.empty((n, len(pct)))\n",
    "\n",
    "for i, ens in enumerate(ensembles):\n",
    "    data[i] = np.percentile(ens, pct)\n",
    "    \n",
    "x = np.arange(n)\n",
    "f90 = plt.fill_between(x, data[:, 1], data[:, 2], alpha=0.2)\n",
    "f67 = plt.fill_between(x, data[:, 3], data[:, 4], alpha=0.5)\n",
    "plt.plot(x, data[:, 0], label='model')\n",
    "\n",
    "# add truth\n",
    "mi, ma = plt.xlim()\n",
    "truth = posterior.ppf(np.array(pct)/100.) # quantiles\n",
    "plt.hlines(truth[0], mi, ma, linestyle='-', label='truth', color='red')\n",
    "plt.hlines(truth[[1,2]], mi, ma, linestyle=':', color='red')\n",
    "plt.hlines(truth[[3,4]], mi, ma, linestyle='--', color='red')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.xlim([mi, ma])\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Quantiles\")\n",
    "\n",
    "# also plot alpha\n",
    "fig = plt.figure()\n",
    "plt.plot(x, alphas, label='alpha')\n",
    "plt.legend(frameon=False, loc=\"upper left\")\n",
    "plt.title('Alpha value to check convergence')\n",
    "plt.grid()\n",
    "fig.set_figheight(3)\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Iteration number\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
